### Base: This base setting will be inherited by all the expid configs.
Base:
    model_root: './checkpoints/'
    num_workers: 3
    verbose: 1
    early_stop_patience: 2
    pickle_feature_encoder: True
    save_best_only: True
    eval_steps: null
    debug_mode: False
    group_id: null
    use_features: null
    feature_specs: null
    feature_config: null

### ModelName_default: This is a config template for hyper-tuning use
PLE_default:
    model: PLE
    dataset_id: TBD
    loss: ['binary_crossentropy','binary_crossentropy']
    metrics: ['logloss', 'AUC']
    task: ['binary_classification','binary_classification']
    num_tasks: 2
    optimizer: adam
    learning_rate: 1.e-3
    num_layers: 1 
    num_shared_experts: 8
    num_specific_experts: 1
    expert_hidden_units: [512,256,128]
    gate_hidden_units: [128, 64]
    tower_hidden_units: [128, 64]
    hidden_activations: relu
    net_regularizer: 0
    embedding_regularizer: 1.e-6
    batch_norm: False
    net_dropout: 0
    batch_size: 128
    embedding_dim: 64
    epochs: 50
    shuffle: True
    seed: 2023
    monitor: 'AUC'
    monitor_mode: 'max'

### ModelName_test: This is a config for test only
PLE_test:
    model: PLE
    dataset_id: tiny_mtl
    loss: ['binary_crossentropy','binary_crossentropy']
    metrics: ['logloss', 'AUC']
    task: ['binary_classification','binary_classification']
    num_tasks: 2
    optimizer: adam
    learning_rate: 1.e-3
    num_layers: 1 
    num_shared_experts: 8
    num_specific_experts: 1
    expert_hidden_units: [512,256,128]
    gate_hidden_units: [128, 64]
    tower_hidden_units: [128, 64]
    hidden_activations: relu
    net_regularizer: 0
    embedding_regularizer: 1.e-6
    batch_norm: False
    net_dropout: 0
    batch_size: 128
    embedding_dim: 64
    epochs: 1
    shuffle: True
    seed: 2023
    monitor: 'AUC'
    monitor_mode: 'max'
